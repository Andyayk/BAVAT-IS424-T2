{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Before closing, go to Cell > All Output > Clear to keep file size small.\n",
    "\n",
    "Also make sure this jupyter notebook file is opened using the following command:\n",
    "\n",
    "jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import all libraries</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, json, gensim, datetime, time, random\n",
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "pd.set_option('display.max_columns', 100000)\n",
    "pd.set_option('display.max_rows', 100000)\n",
    "\n",
    "stop_list = nltk.corpus.stopwords.words('english') #creating list of stopwords\n",
    "stemmer = nltk.stem.porter.PorterStemmer() #stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Feature Selection (Overview)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_csv(\"tmdb_All_movies.csv\", header = 0)\n",
    "corpuslist = all_df[\"overview\"]\n",
    "corpuslist = corpuslist.replace([np.inf, -np.inf, np.nan], \" \") #removing infinite/nan values\n",
    "\n",
    "overviews = []\n",
    "\n",
    "def processOverview(overview):\n",
    "    eachwordinoverview = nltk.word_tokenize(overview)\n",
    "    text1 = [w.lower() for w in eachwordinoverview] #lower case the words\n",
    "    text2 = [w for w in text1 if re.search('^[a-z]+$', w)] #removing special characters and numbers\n",
    "    text3 = [w for w in text2 if w not in stop_list] #removing words in stop list\n",
    "    text4 = [stemmer.stem(w) for w in text3] #changing the words into its root form\n",
    "    \n",
    "    return text4   \n",
    "\n",
    "for overview in corpuslist:\n",
    "    eachwordinoverview = processOverview(overview)\n",
    "    overviews += eachwordinoverview\n",
    "\n",
    "corpuslist = all_df[\"tagline\"]\n",
    "corpuslist = corpuslist.replace([np.inf, -np.inf, np.nan], \" \") #removing infinite/nan values\n",
    "\n",
    "for tagline in corpuslist:\n",
    "    eachwordinoverview = processOverview(tagline)\n",
    "    overviews += eachwordinoverview\n",
    "\n",
    "corpuslist = overviews\n",
    "\n",
    "#The below code is for the feature set definition. We are using only top 5000 words as our features \n",
    "fdist = nltk.FreqDist(w.lower() for w in corpuslist)\n",
    "\n",
    "totaluniquewords = 0\n",
    "for word in fdist:\n",
    "    totaluniquewords+=1\n",
    "print(\"Total Unique Words:\", totaluniquewords)\n",
    "\n",
    "datasize = 5000\n",
    "\n",
    "mostcommonwords = fdist.most_common()[:datasize] #top 5k\n",
    "mostcommonwords = [w[0] for w in mostcommonwords]\n",
    "\n",
    "print(\"Total Most Common Words:\", len(mostcommonwords))\n",
    "print(mostcommonwords[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Feature Selection (Production Companies)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_csv(\"tmdb_All_movies.csv\", header = 0)\n",
    "corpuslist = all_df[\"production_companies\"]\n",
    "\n",
    "productioncompanies = []\n",
    "\n",
    "def readJSON(company, keyword):\n",
    "    list = []\n",
    "    jsonobj = json.loads(company) #loading the json string into a json object\n",
    "    \n",
    "    for jsonelement in jsonobj:\n",
    "        keywordelement = jsonelement[keyword] #getting each word out\n",
    "        keywordelement = keywordelement.lower().replace(\" \", \"\") #lower case the words, removing all whitespaces \n",
    "        if re.search('^[a-z]+$', keywordelement):\n",
    "            list.append(keywordelement)\n",
    "            \n",
    "    return list\n",
    "\n",
    "for company in corpuslist:\n",
    "    eachwordincompany = readJSON(company, 'name')\n",
    "    productioncompanies += eachwordincompany\n",
    "\n",
    "corpuslist = productioncompanies\n",
    "    \n",
    "#The below code is for the feature set definition. We are using only top 1000 companies as our features \n",
    "fdist2 = nltk.FreqDist(w.lower() for w in corpuslist)\n",
    "\n",
    "totaluniquecompanies = 0\n",
    "for word in fdist2:\n",
    "    totaluniquecompanies+=1\n",
    "print(\"Total Unique Companies:\", totaluniquecompanies)\n",
    "\n",
    "datasize = 1000\n",
    "\n",
    "mostcommoncompanies = fdist2.most_common()[:datasize] #top 1k\n",
    "mostcommoncompanies = [w[0] for w in mostcommoncompanies]\n",
    "\n",
    "print(\"Total Most Common Companies:\", len(mostcommoncompanies))\n",
    "print(mostcommoncompanies[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Procesing tmdb_All_movies.csv (Movies)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idlist = []\n",
    "titlelist = []\n",
    "genreslist = []\n",
    "overviewlist = []\n",
    "productioncompanieslist = []\n",
    "\n",
    "def readJSON(row, rownumber, list, keyword):\n",
    "    jsonstring = row[rownumber]\n",
    "\n",
    "    jsonobj = json.loads(jsonstring) #loading the json string into a json object\n",
    "    \n",
    "    for jsonelement in jsonobj:\n",
    "        keywordelement = jsonelement[keyword] #getting each word out\n",
    "        keywordelement = keywordelement.lower().replace(\" \", \"\") #lower case the words, removing all whitespaces \n",
    "        if re.search('^[a-z]+$', keywordelement):\n",
    "            if rownumber == 8: #production companies\n",
    "                if keywordelement in mostcommoncompanies: #selecting top 1k companies as our features\n",
    "                    list.append(keywordelement)\n",
    "            else: #genres\n",
    "                list.append(keywordelement)\n",
    "    \n",
    "    list = ' '.join(list) #changing list into a string\n",
    "    \n",
    "    return list\n",
    "    \n",
    "def processText(row, rownumber1, rownumber2, list):\n",
    "    text = nltk.word_tokenize(row[rownumber1] + \" \" + row[rownumber2]) #contains overview and tagline\n",
    "    text1 = [w.lower() for w in text] #lower case the words\n",
    "    text2 = [w for w in text1 if re.search('^[a-z]+$', w)] #removing special characters and numbers\n",
    "    text3 = [w for w in text2 if w not in stop_list] #removing words in stop list\n",
    "    text4 = [stemmer.stem(w) for w in text3] #changing the words into its root form\n",
    "\n",
    "    #text5 = text4 #no feature selection\n",
    "    text5 = [w for w in text4 if w in mostcommonwords] #selecting top 5k words as our features\n",
    "    \n",
    "    list = ' '.join(text5) #changing list into a string\n",
    "    \n",
    "    return list\n",
    "    \n",
    "#read file\n",
    "with open('tmdb_All_movies.csv', encoding='utf-8') as csv_file: #change accordingly\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        try:        \n",
    "            if line_count == 0:\n",
    "                print(f'Column names are {\", \".join(row)}')\n",
    "                line_count += 1\n",
    "            else: #if line_count != 1000:\n",
    "                genres = []\n",
    "                productioncompanies = []\n",
    "                overview = []\n",
    "                      \n",
    "                if row[1] != \"\":\n",
    "                    #handle json genres\n",
    "                    genres = readJSON(row, 1, genres, 'name')\n",
    "\n",
    "                #handle overview\n",
    "                overview = processText(row, 6, 15, overview)\n",
    "\n",
    "                if row[8] != \"\":\n",
    "                    #handle json productioncompanies\n",
    "                    productioncompanies = readJSON(row, 8, productioncompanies, 'name')\n",
    "\n",
    "                idlist.append(row[3]) #id\n",
    "                titlelist.append(row[16]) #title\n",
    "                genreslist.append(genres)  \n",
    "                overviewlist.append(overview)\n",
    "                productioncompanieslist.append(productioncompanies)\n",
    "\n",
    "                line_count += 1\n",
    "        except Exception as e:\n",
    "            print(f'Row: {line_count} has Exception' + str(e))    \n",
    "            line_count += 1          \n",
    "    print(f'Processed {line_count} lines.')              \n",
    "\n",
    "def convertToDataframe(listofwords, idlist, titlelist):\n",
    "    vectorizer = TfidfVectorizer(analyzer='word') #tfidf\n",
    "    words_tfidf = vectorizer.fit_transform(listofwords) #tfidf\n",
    "\n",
    "    tablecolumns = []                      \n",
    "    tablecolumns.append(vectorizer.get_feature_names()) #adding column headers\n",
    "\n",
    "    df = pd.DataFrame(words_tfidf.toarray(), columns=tablecolumns) #creating dataframe\n",
    "\n",
    "    df['id'] = idlist\n",
    "    df['title'] = titlelist\n",
    "                      \n",
    "    return df\n",
    "\n",
    "dfgenres = convertToDataframe(genreslist, idlist, titlelist)\n",
    "print(dfgenres.head(10))\n",
    "                      \n",
    "dfoverview = convertToDataframe(overviewlist, idlist, titlelist)\n",
    "print(dfoverview.head(10))\n",
    "                      \n",
    "dfproductioncompanies = convertToDataframe(productioncompanieslist, idlist, titlelist)\n",
    "print(dfproductioncompanies.head(10)) \n",
    "\n",
    "print('\\nOutput Success!')                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Save to Dataframe</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfgenres.to_pickle(\"dfgenres\")\n",
    "dfoverview.to_pickle(\"dfoverview\")\n",
    "dfproductioncompanies.to_pickle(\"dfproductioncompaniesmostcommon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Processing tmdb_All_credits.csv (Credits)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idlist = []\n",
    "castslist = []\n",
    "directorslist = []\n",
    "\n",
    "def readJSON(row, rownumber, list, keyword):\n",
    "    jsonstring = row[rownumber]\n",
    "\n",
    "    jsonobj = json.loads(jsonstring) #loading the json string into a json object\n",
    "    \n",
    "    for jsonelement in jsonobj:\n",
    "        keywordelement = jsonelement[keyword] #getting each word out\n",
    "        keywordelement = keywordelement.lower().replace(\" \", \"\") #lower case the words, removing all whitespaces \n",
    "        list.append(keywordelement) \n",
    "    \n",
    "    list = ' '.join(list) #changing list into a string\n",
    "    \n",
    "    return list\n",
    "\n",
    "#read top 1k actors and actresses file\n",
    "topcasts = pd.read_csv(\"top_actors_actresses.csv\", encoding=\"ISO-8859-1\") \n",
    "namesoftopcasts = topcasts['Name'].values.tolist()\n",
    "processedcastnames = []\n",
    "\n",
    "for name in namesoftopcasts:\n",
    "    name = name.lower().replace(\" \", \"\") #lower case the words, removing all whitespaces \n",
    "    processedcastnames.append(name)  \n",
    "    \n",
    "#read top directors file\n",
    "topdirectors = pd.read_csv(\"top_directors.csv\", encoding=\"ISO-8859-1\") \n",
    "namesoftopdirectors = topdirectors['Name'].values.tolist()\n",
    "processeddirectorsnames = []\n",
    "\n",
    "for name in namesoftopdirectors:\n",
    "    name = name.lower().replace(\" \", \"\") #lower case the words, removing all whitespaces \n",
    "    processeddirectorsnames.append(name)      \n",
    "\n",
    "#read file\n",
    "with open('tmdb_All_credits.csv', encoding='utf-8') as csv_file: #change accordingly\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        try:\n",
    "            if line_count == 0:\n",
    "                print(f'Column names are {\", \".join(row)}')\n",
    "                line_count += 1\n",
    "            else: #if line_count != 1000:\n",
    "                casts = []\n",
    "                directors = []\n",
    "                      \n",
    "                if row[1] != \"\":\n",
    "                    #handle json casts\n",
    "                    casts = readJSON(row, 1, casts, 'name')                      \n",
    "                \n",
    "                #handle json directors \n",
    "                if row[2] != \"\":      \n",
    "                    jsonstring = row[2]\n",
    "\n",
    "                    jsonobj = json.loads(jsonstring) #loading the json string into a json object\n",
    "\n",
    "                    for jsonelement in jsonobj:\n",
    "                        job = jsonelement['job'] #getting each word out\n",
    "                        if job == \"Director\": \n",
    "                            director = jsonelement['name'].lower().replace(\" \", \"\") #lower case the words, removing all whitespaces \n",
    "                            directors.append(director) \n",
    "\n",
    "                    directors = ' '.join(directors) #changing list into a string                      \n",
    "                \n",
    "                idlist.append(row[0]) #id                      \n",
    "                castslist.append(casts)\n",
    "                directorslist.append(directors)         \n",
    "\n",
    "                line_count += 1\n",
    "        except Exception as e:\n",
    "            print(f'Row: {line_count} has Exception' + str(e))    \n",
    "            line_count += 1   \n",
    "\n",
    "    print(f'Processed {line_count} lines.')\n",
    "                  \n",
    "def convertToDataframe(listofwords, idlist):\n",
    "    vectorizer = TfidfVectorizer(analyzer='word') #tfidf\n",
    "    words_tfidf = vectorizer.fit_transform(listofwords) #tfidf\n",
    "\n",
    "    tablecolumns = []                      \n",
    "    tablecolumns.append(vectorizer.get_feature_names()) #adding column headers\n",
    "\n",
    "    df = pd.DataFrame(words_tfidf.toarray(), columns=tablecolumns) #creating dataframe\n",
    "\n",
    "    df['id'] = idlist\n",
    "                      \n",
    "    return df\n",
    "\n",
    "processedcastnames.append('id')\n",
    "processeddirectorsnames.append('id')\n",
    "                      \n",
    "dfcasts = convertToDataframe(castslist, idlist)\n",
    "dfcasts2 = dfcasts.loc[:, processedcastnames]\n",
    "\n",
    "print(dfcasts2.head(10))\n",
    "                      \n",
    "dfdirectors = convertToDataframe(directorslist, idlist)\n",
    "dfdirectors2 = dfdirectors.loc[:, processeddirectorsnames]\n",
    "\n",
    "print(dfdirectors2.head(10))                        \n",
    "\n",
    "print('\\nOutput Success!')                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Save to Dataframe</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcasts2.to_pickle(\"dfcasts\")\n",
    "dfdirectors2.to_pickle(\"dfdirectors\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

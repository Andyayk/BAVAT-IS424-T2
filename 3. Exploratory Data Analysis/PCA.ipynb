{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Useful Links</h4>\n",
    "<ul>\n",
    "<li>\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA\">Incremental PCA</a></li>\n",
    "<li><a href=\"https://stackabuse.com/implementing-pca-in-python-with-scikit-learn/\">PCA Implementation</a></li>\n",
    "<li><a href=\"https://stackoverflow.com/questions/44334950/how-to-use-sklearns-incrementalpca-partial-fit\">Partial Fit</a></li>\n",
    "</ul>\n",
    "\n",
    "<h4>Ways to implement PCA</h4>\n",
    "<ul>\n",
    "<li>PCA vs IncrementalPCA (latter for bigger data)</li>\n",
    "<li>Fit vs Partial Fit (latter for bigger data)</li>\n",
    "<li>Fit_transform vs Fit & transform (latter for different datasets/bigger data)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# %matplotlib inline\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "dfcasts = pd.read_pickle(\"../2. Data Preprocessing/dfcasts\")\n",
    "dfcasts.columns = [x[0] for x in dfcasts.columns]\n",
    "# print(dfcasts.columns.values)\n",
    "\n",
    "dfdirectors = pd.read_pickle(\"../2. Data Preprocessing/dfdirectors\")\n",
    "dfdirectors.columns = [x[0] for x in dfdirectors.columns]\n",
    "# print(dfdirectors.columns.values)\n",
    "\n",
    "dfgenres = pd.read_pickle(\"../2. Data Preprocessing/dfgenres\")\n",
    "dfgenres.columns = [x[0] for x in dfgenres.columns]\n",
    "# print(dfgenres.columns.values)\n",
    "\n",
    "dfoverview = pd.read_pickle(\"../2. Data Preprocessing/dfoverview\")\n",
    "dfoverview.columns = [x[0] for x in dfoverview.columns]\n",
    "# print(dfoverview.columns.values)\n",
    "\n",
    "dfproductioncompanies = pd.read_pickle(\"../2. Data Preprocessing/dfproductioncompanies\")\n",
    "dfproductioncompanies.columns = [x[0] for x in dfproductioncompanies.columns]\n",
    "# print(dfproductioncompanies.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_casts = dfcasts.drop(['id'], 1)\n",
    "X_casts.dropna(inplace=True)\n",
    "print(X_casts.shape)\n",
    "\n",
    "X_directors = dfdirectors.drop(['id'], 1)\n",
    "X_directors.dropna(inplace=True)\n",
    "print(X_directors.shape)\n",
    "\n",
    "X_genres = dfgenres.drop(['id', 'title'], 1)\n",
    "X_genres.dropna(inplace=True)\n",
    "print(X_genres.shape)\n",
    "\n",
    "X_overview = dfoverview.drop(['id', 'title'], 1)\n",
    "X_overview.dropna(inplace=True)\n",
    "print(X_overview.shape)\n",
    "\n",
    "X_productioncompanies = dfproductioncompanies.drop(['id', 'title'], 1)\n",
    "X_productioncompanies.dropna(inplace=True)\n",
    "print(X_productioncompanies.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_casts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA vs IncrementalPCA (latter for bigger data)\n",
    "# Fit vs Partial Fit (latter for bigger data)\n",
    "# Fit_transform vs Fit & transform (latter for different datasets/bigger data)\n",
    "\n",
    "num_rows = X_overview.shape[0] # total number of rows in data\n",
    "chunk_size = 1000 # how many rows at a time to feed ipca\n",
    "ipca = IncrementalPCA()\n",
    "for i in range(0, num_rows//chunk_size):\n",
    "    ipca.partial_fit(X_overview[i*chunk_size : (i+1)*chunk_size])\n",
    "\n",
    "out = []\n",
    "for i in range(0, num_rows//chunk_size):\n",
    "    out[i*chunk_size:(i+1) * chunk_size] = ipca.transform(X_overview[i*chunk_size : (i+1)*chunk_size])\n",
    "\n",
    "print(ipca.explained_variance_ratio_.cumsum())\n",
    "    \n",
    "# ipca1 = IncrementalPCA() # batch size none\n",
    "# ipca1.fit(X_casts)\n",
    "# X_casts_transformed = ipca1.transform(X_casts)\n",
    "# print(X_casts_transformed.shape)\n",
    "# print (X_casts_transformed)\n",
    "\n",
    "# # pca1 = PCA()\n",
    "# # pca1.fit_transform(X_casts)\n",
    "# casts_explained_variance = ipca1.explained_variance_ratio_\n",
    "# print(casts_explained_variance)\n",
    "\n",
    "# print(len(casts_explained_variance))\n",
    "# plt.plot(casts_explained_variance[0:3]) # adjust values to zoom in/out of graph\n",
    "# plt.show()\n",
    "# # sum(casts_explained_variance[:1])\n",
    "# print(casts_explained_variance[0])\n",
    "\n",
    "# ipca1 = IncrementalPCA(copy=True, n_components=1, whiten=False) # batch size none\n",
    "# ipca1.fit(X_casts)\n",
    "# X_casts_transformed = ipca1.transform(X_casts)\n",
    "# print(X_casts_transformed.shape)\n",
    "# print (X_casts_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(out) #data rate limit exceeded\n",
    "overview_explained_variance = ipca.explained_variance_ratio_\n",
    "print(overview_explained_variance)\n",
    "print(len(overview_explained_variance.cumsum()))\n",
    "plt.plot(overview_explained_variance[0:200]) # adjust values to zoom in/out of graph\n",
    "plt.show()\n",
    "sum(overview_explained_variance[:1])\n",
    "# print(overview_explained_variance[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Casts</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca1 = PCA()\n",
    "pca1.fit_transform(X_casts)\n",
    "casts_explained_variance = pca1.explained_variance_ratio_\n",
    "print(casts_explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(casts_explained_variance))\n",
    "\n",
    "plt.plot(casts_explained_variance[0:3]) # adjust values to zoom in/out of graph\n",
    "plt.show()\n",
    "\n",
    "# sum(casts_explained_variance[:1])\n",
    "print(casts_explained_variance[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipca1 = IncrementalPCA(copy=True, n_components=1, whiten=False) # batch size none\n",
    "ipca1.fit(X_casts)\n",
    "X_casts_transformed = ipca1.transform(X_casts)\n",
    "print(X_casts_transformed.shape)\n",
    "print (X_casts_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Directors</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca2 = PCA()\n",
    "pca2.fit_transform(X_directors)\n",
    "directors_explained_variance = pca2.explained_variance_ratio_\n",
    "print(directors_explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(directors_explained_variance))\n",
    "\n",
    "plt.plot(directors_explained_variance[0:3]) # adjust values to zoom in/out of graph\n",
    "plt.show()\n",
    "\n",
    "# sum(directors_explained_variance[:1])\n",
    "print(directors_explained_variance[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipca2 = IncrementalPCA(copy=True, n_components=1, whiten=False) # batch size none\n",
    "ipca2.fit(X_directors)\n",
    "X_directors_transformed = ipca2.transform(X_directors)\n",
    "print(X_directors_transformed.shape)\n",
    "print (X_directors_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Genres</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca3 = PCA()\n",
    "pca3.fit_transform(X_genres)\n",
    "genres_explained_variance = pca3.explained_variance_ratio_\n",
    "print(genres_explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(genres_explained_variance))\n",
    "\n",
    "plt.plot(genres_explained_variance[0:3]) # adjust values to zoom in/out of graph\n",
    "plt.show()\n",
    "\n",
    "# sum(genres_explained_variance[:1])\n",
    "print(genres_explained_variance[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipca3 = IncrementalPCA(copy=True, n_components=1, whiten=False) # batch size none\n",
    "ipca3.fit(X_genres)\n",
    "X_genres_transformed = ipca3.transform(X_genres)\n",
    "print(X_genres_transformed.shape)\n",
    "print (X_genres_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Overview</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca4 = PCA()\n",
    "pca4.fit_transform(X_overview)\n",
    "overview_explained_variance = pca4.explained_variance_ratio_\n",
    "print(overview_explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(overview_explained_variance))\n",
    "\n",
    "plt.plot(overview_explained_variance[0:3]) # adjust values to zoom in/out of graph\n",
    "plt.show()\n",
    "\n",
    "# sum(overview_explained_variance[:1])\n",
    "print(overview_explained_variance[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipca4 = IncrementalPCA(copy=True, n_components=1, whiten=False) # batch size none\n",
    "ipca4.fit(X_overview)\n",
    "X_overview_transformed = ipca4.transform(X_overview)\n",
    "print(X_overview_transformed.shape)\n",
    "print (X_overview_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Production Companies</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca5 = PCA(standardize=False)\n",
    "pca5.fit_transform(X_productioncompanies)\n",
    "productioncompanies_explained_variance = pca5.explained_variance_ratio_\n",
    "print(productioncompanies_explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(productioncompanies_explained_variance))\n",
    "\n",
    "plt.plot(productioncompanies_explained_variance[0:3]) # adjust values to zoom in/out of graph\n",
    "plt.show()\n",
    "\n",
    "# sum(productioncompanies_explained_variance[:1])\n",
    "print(productioncompanies_explained_variance[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipca5 = IncrementalPCA(copy=True, n_components=1, whiten=False) # batch size none\n",
    "ipca5.fit(X_productioncompanies)\n",
    "X_productioncompanies_transformed = ipca5.transform(X_productioncompanies)\n",
    "print(X_productioncompanies_transformed.shape)\n",
    "print (X_productioncompanies_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Unused Code</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_df = pd.read_csv('../2. Data Preprocessing/tmdb_All_movies.csv')\n",
    "\n",
    "# revenue = pd.read_pickle(\"../2. Data Preprocessing/processed_data\")\n",
    "# revenue.columns = [x[0] for x in revenue.columns]\n",
    "# revenue = revenue[\"revenue\"] # y\n",
    "# # print(revenue)\n",
    "# y = revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer = IncrementalPCA(n_components=7, batch_size=200)\n",
    "# # either partially fit on smaller batches of data\n",
    "# transformer.partial_fit(X[:100, :])\n",
    "# IncrementalPCA(batch_size=200, copy=True, n_components=7, whiten=False)\n",
    "# # or let the fit function itself divide the data into batches\n",
    "# X_transformed = transformer.fit_transform(X)\n",
    "# X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df = my_df.loc[:, ['overview','revenue']]\n",
    "my_df.dropna(inplace=True)\n",
    "# my_df.reset_index(drop=True,inplace=True)\n",
    "my_df.info()\n",
    "my_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = my_df.overview\n",
    "y = my_df.revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "import time\n",
    "\n",
    "corpus = my_df[\"overview\"].values\n",
    "# print(corpus)\n",
    "\n",
    "start = time. time()\n",
    "stop_list = nltk.corpus.stopwords.words('english')\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "def corpus2docs(corpus):\n",
    "    # corpus is a object returned by load_corpus that represents a corpus.\n",
    "    docs1 = []\n",
    "\n",
    "    for title in corpus:\n",
    "        doc = nltk.word_tokenize(title)\n",
    "        docs1.append(doc)\n",
    "    docs2 = [[w.lower() for w in doc] for doc in docs1]\n",
    "    docs3 = [[w for w in doc if re.search('^[a-z]+$', w)] for doc in docs2]\n",
    "    docs4 = [[w for w in doc if w not in stop_list] for doc in docs3]\n",
    "    docs5 = [[stemmer.stem(w) for w in doc] for doc in docs4]\n",
    "    return docs5\n",
    "\n",
    "docs = corpus2docs(corpus)\n",
    "end = time. time()\n",
    "print(end - start)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "sent = \"Io andiamo to the beach with my amico.\"\n",
    "\" \".join(w for w in nltk.wordpunct_tokenize(sent) \\\n",
    "         if w.lower() in words or not w.isalpha())\n",
    "# 'Io to the beach with my'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from sklearn.cross_validation import train_test_split\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# SEED = 2000\n",
    "# x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size=.02, random_state=SEED)\n",
    "# x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"train set\")\n",
    "# print(len(x_train))\n",
    "# print(\"validation set\")\n",
    "# print(len(x_validation))\n",
    "# print(\"test set\")\n",
    "# print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

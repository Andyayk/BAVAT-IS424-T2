{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import all libraries and reading explored data into Dataframe</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T13:49:14.197888Z",
     "start_time": "2018-11-02T13:49:12.900261Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "\n",
    "#General libraries needed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "#Libraries for data pre-processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Libraries for data pre-processing (Log Loss)\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "#For Decision Tree implementation\n",
    "from scipy.stats import entropy\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "#For KNN implementation\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#For Bagging implementation\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "#For AdaBoost implementation\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#For Random Forest implementation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#For Baseline implementation\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "#For Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#For Ensemble\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#Settings\n",
    "%matplotlib inline\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "sns.set()\n",
    "\n",
    "def printModelAccuracy(y_test, y_pred):\n",
    "    # Find the confusion matrix of the result\n",
    "    cm = pd.DataFrame(confusion_matrix(y_test, y_pred, labels=[1, 2, 3, 4, 5]), \\\n",
    "        index=['true:1', 'true:2', 'true:3', 'true:4', 'true:5'], \n",
    "        columns=['pred:1', 'pred:2', 'pred:3', 'pred:4', 'pred:5'])\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Find the accuracy and F1 score of the result\n",
    "    asr = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='micro')\n",
    "    print(\"Accuracy:\", asr)\n",
    "    print(\"F1:\", f1)\n",
    "    \"\"\"\n",
    "    # Log loss\n",
    "    score = log_loss(y_test, y_pred)\n",
    "    print(\"Log Loss:\", score)\n",
    "    \"\"\"\n",
    "    \n",
    "# Read from dataframe    \n",
    "X_test = pd.read_pickle(\"X_test\")\n",
    "X_train = pd.read_pickle(\"X_train\")\n",
    "y_test = pd.read_pickle(\"y_test\")\n",
    "y_train = pd.read_pickle(\"y_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Random Forest</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "        pred:1  pred:2  pred:3  pred:4  pred:5\n",
      "true:1     344     230     171      32      12\n",
      "true:2     285     257     184      48      28\n",
      "true:3     237     193     167      74     115\n",
      "true:4     159     131     118      74     317\n",
      "true:5     101      49      52      34     500\n",
      "Accuracy: 0.3430470347648262\n",
      "F1: 0.3430470347648262\n",
      "Best Parameters: {'criterion': 'entropy', 'n_estimators': 1400}\n"
     ]
    }
   ],
   "source": [
    "#Instantiate model\n",
    "randomforest = RandomForestClassifier()\n",
    "\n",
    "parameters = { \n",
    "    'n_estimators': [800, 1400], #500, 900 1000 not good\n",
    "    'criterion': ['entropy'] #gini not good\n",
    "}\n",
    "\n",
    "#Fit the training feature Xs and training label Ys\n",
    "randomforest = GridSearchCV(randomforest, cv=5, param_grid=parameters, scoring='f1_micro')\n",
    "randomforest.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "#Use the trained model to predict the test data\n",
    "y_pred = randomforest.predict(X_test)\n",
    "\n",
    "# Find the confusion matrix, the accuracy, and F1 score of the result\n",
    "printModelAccuracy(y_test, y_pred)\n",
    "\n",
    "# Best hyperparameters to use for model\n",
    "print(\"Best Parameters:\", randomforest.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Logistic Regression</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "        pred:1  pred:2  pred:3  pred:4  pred:5\n",
      "true:1     458     203     105      21       2\n",
      "true:2     374     256     133      31       8\n",
      "true:3     277     194     186      99      30\n",
      "true:4     174     122     105     212     186\n",
      "true:5     104      48      49     117     418\n",
      "Accuracy: 0.3911042944785276\n",
      "F1: 0.39110429447852757\n",
      "Best Parameters: {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "#create a new logistic regression model ‘lbfgs’, ‘sag’ and ‘newton-cg’ solvers.\n",
    "log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "\n",
    "parameters = {\n",
    "    'C': [0.1] \n",
    "}\n",
    "\n",
    "#Fit the training feature Xs and training label Ys\n",
    "log_reg = GridSearchCV(log_reg, cv=5, param_grid=parameters, scoring='f1_micro')\n",
    "log_reg.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "#Use the trained model to predict the test data\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Find the confusion matrix, the accuracy, and F1 score of the result\n",
    "printModelAccuracy(y_test, y_pred)\n",
    "\n",
    "# Best hyperparameters to use for model\n",
    "print(\"Best Parameters:\", log_reg.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Ensemble (Stacking with all models)</h1>\n",
    "\n",
    "As I mentioned in lecture, it is possible to ensemble different models. So how can we do that in python? Check out the following link and try it for your project!:\n",
    "https://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "        pred:1  pred:2  pred:3  pred:4  pred:5\n",
      "true:1     567     170      36      11       5\n",
      "true:2     500     197      70      24      11\n",
      "true:3     378     134     139      95      40\n",
      "true:4     224     105     107     154     209\n",
      "true:5     134      45      30      67     460\n",
      "Accuracy: 0.38778118609406953\n",
      "F1: 0.38778118609406953\n"
     ]
    }
   ],
   "source": [
    "#Adaboost(DecisionTree) with best parameters\n",
    "adaboostTree = AdaBoostClassifier(DecisionTreeClassifier(criterion='gini', max_depth=9, splitter='random'), learning_rate=0.001, n_estimators=300)\n",
    "adaboostTree.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "#random forest with best parameters\n",
    "randomforest = RandomForestClassifier(criterion='entropy', n_estimators=1400)\n",
    "randomforest.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "#create a new logistic regression model ‘lbfgs’, ‘sag’ and ‘newton-cg’ solvers.\n",
    "log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, C=0.1)\n",
    "\n",
    "#create a dictionary of our models\n",
    "estimators=[('adaboostTree', adaboostTree),\n",
    "            ('randomforest', randomforest), \n",
    "            ('log_reg', log_reg)]\n",
    "\n",
    "#create our voting classifier, inputting our models, voting hard means asking classifers to make predictions by majority vote\n",
    "ensemble = VotingClassifier(estimators, voting='hard')\n",
    "\n",
    "#fit model to training data\n",
    "ensemble.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "#Use the trained model to predict the test data\n",
    "y_pred = ensemble.predict(X_test)\n",
    "\n",
    "# Find the confusion matrix, the accuracy, and F1 score of the result\n",
    "printModelAccuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Save the best model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save classifier\n",
    "save_classifier = open(\"logregmodel.pickle\",\"wb\") #binary write\n",
    "pickle.dump(log_reg, save_classifier)\n",
    "save_classifier.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

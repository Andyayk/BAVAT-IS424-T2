{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import all libraries and reading explored data into Dataframe</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T13:49:14.197888Z",
     "start_time": "2018-11-02T13:49:12.900261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "budget                 float64\n",
      "revenue                float64\n",
      "weekday                float64\n",
      "day                    float64\n",
      "month                  float64\n",
      "year                   float64\n",
      "runtime                float64\n",
      "vote_average           float64\n",
      "vote_count             float64\n",
      "weighted_rating        float64\n",
      "log_revenue            float64\n",
      "log_budget             float64\n",
      "log_runtime            float64\n",
      "log_vote_average       float64\n",
      "log_vote_count         float64\n",
      "log_weighted_rating    float64\n",
      "bin                    float64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>budget</th>\n",
       "      <th>revenue</th>\n",
       "      <th>weekday</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>runtime</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "      <th>weighted_rating</th>\n",
       "      <th>log_revenue</th>\n",
       "      <th>log_budget</th>\n",
       "      <th>log_runtime</th>\n",
       "      <th>log_vote_average</th>\n",
       "      <th>log_vote_count</th>\n",
       "      <th>log_weighted_rating</th>\n",
       "      <th>bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>0.00</td>\n",
       "      <td>26982100.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2017.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>17.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.95</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12657</th>\n",
       "      <td>65000000.00</td>\n",
       "      <td>40002112.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>1998.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>5.20</td>\n",
       "      <td>670.00</td>\n",
       "      <td>6.95</td>\n",
       "      <td>17.50</td>\n",
       "      <td>17.99</td>\n",
       "      <td>4.61</td>\n",
       "      <td>1.65</td>\n",
       "      <td>6.51</td>\n",
       "      <td>1.94</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15497</th>\n",
       "      <td>30000000.00</td>\n",
       "      <td>31148328.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>2008.00</td>\n",
       "      <td>130.00</td>\n",
       "      <td>6.40</td>\n",
       "      <td>392.00</td>\n",
       "      <td>6.99</td>\n",
       "      <td>17.25</td>\n",
       "      <td>17.22</td>\n",
       "      <td>4.87</td>\n",
       "      <td>1.86</td>\n",
       "      <td>5.97</td>\n",
       "      <td>1.94</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21030</th>\n",
       "      <td>25000000.00</td>\n",
       "      <td>71009334.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>2013.00</td>\n",
       "      <td>117.00</td>\n",
       "      <td>5.10</td>\n",
       "      <td>1025.00</td>\n",
       "      <td>6.93</td>\n",
       "      <td>18.08</td>\n",
       "      <td>17.03</td>\n",
       "      <td>4.76</td>\n",
       "      <td>1.63</td>\n",
       "      <td>6.93</td>\n",
       "      <td>1.94</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20995</th>\n",
       "      <td>0.00</td>\n",
       "      <td>7965682.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>1986.00</td>\n",
       "      <td>84.00</td>\n",
       "      <td>5.10</td>\n",
       "      <td>5.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>15.89</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.43</td>\n",
       "      <td>1.63</td>\n",
       "      <td>1.61</td>\n",
       "      <td>1.95</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           budget     revenue  weekday   day  month    year  runtime  \\\n",
       "784          0.00 26982100.00     7.00 29.00   1.00 2017.00     9.00   \n",
       "12657 65000000.00 40002112.00     5.00 13.00  11.00 1998.00   100.00   \n",
       "15497 30000000.00 31148328.00     2.00  9.00   9.00 2008.00   130.00   \n",
       "21030 25000000.00 71009334.00     5.00 25.00  10.00 2013.00   117.00   \n",
       "20995        0.00  7965682.00     1.00  1.00  12.00 1986.00    84.00   \n",
       "\n",
       "       vote_average  vote_count  weighted_rating  log_revenue  log_budget  \\\n",
       "784            5.00        1.00             7.00        17.11        0.00   \n",
       "12657          5.20      670.00             6.95        17.50       17.99   \n",
       "15497          6.40      392.00             6.99        17.25       17.22   \n",
       "21030          5.10     1025.00             6.93        18.08       17.03   \n",
       "20995          5.10        5.00             7.00        15.89        0.00   \n",
       "\n",
       "       log_runtime  log_vote_average  log_vote_count  log_weighted_rating  bin  \n",
       "784           2.20              1.61            0.00                 1.95 4.00  \n",
       "12657         4.61              1.65            6.51                 1.94 4.00  \n",
       "15497         4.87              1.86            5.97                 1.94 4.00  \n",
       "21030         4.76              1.63            6.93                 1.94 5.00  \n",
       "20995         4.43              1.63            1.61                 1.95 3.00  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import io\n",
    "\n",
    "#General libraries needed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Libraries for data pre-processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Libraries for data pre-processing (Log Loss)\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "#For Decision Tree implementation\n",
    "from scipy.stats import entropy\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "#For KNN implementation\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#For Bagging implementation\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "#For AdaBoost implementation\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#For Random Forest implementation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#For Baseline implementation\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "#For Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#For Ensemble\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#Settings\n",
    "%matplotlib inline\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "sns.set()\n",
    "\n",
    "def printModelAccuracy(y_test, y_pred):\n",
    "    # Find the confusion matrix of the result\n",
    "    cm = pd.DataFrame(confusion_matrix(y_test, y_pred, labels=[1, 2, 3, 4, 5]), \\\n",
    "        index=['true:1', 'true:2', 'true:3', 'true:4', 'true:5'], \n",
    "        columns=['pred:1', 'pred:2', 'pred:3', 'pred:4', 'pred:5'])\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Find the accuracy and F1 score of the result\n",
    "    asr = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    print(\"Accuracy:\", asr)\n",
    "    print(\"F1:\", f1)\n",
    "    \"\"\"\n",
    "    # Log loss\n",
    "    score = log_loss(y_test, y_pred)\n",
    "    print(\"Log Loss:\", score)\n",
    "    \"\"\"\n",
    "    \n",
    "# Read from dataframe\n",
    "dfnum = pd.read_pickle(\"../3. Exploratory Data Analysis/explored_data\")\n",
    "dfnum = df.replace([np.inf, -np.inf, np.nan], 0) #removing infinite/nan values\n",
    "df = dfnum.drop(['id'], 1)\n",
    "\n",
    "# Check the columns using dtypes\n",
    "print(df.dtypes)\n",
    "# Randomly sample 5 records with .sample(5)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree in SKLearn don't take in string well. So we use a label encoder to change that string to a numeric value\n",
    "\"\"\"\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == type(object):\n",
    "        #Create the label encoder\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        #Convert the non numeric data to numeric\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19560, 18)\n",
      "(22775, 942)\n",
      "(22775, 507)\n",
      "(23579, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69129, 1440)\n",
      "budget                 float64\n",
      "revenue                float64\n",
      "weekday                float64\n",
      "day                    float64\n",
      "month                  float64\n",
      "year                   float64\n",
      "runtime                float64\n",
      "vote_average           float64\n",
      "vote_count             float64\n",
      "weighted_rating        float64\n",
      "id                     float64\n",
      "log_revenue            float64\n",
      "log_budget             float64\n",
      "log_runtime            float64\n",
      "log_vote_average       float64\n",
      "log_vote_count         float64\n",
      "log_weighted_rating    float64\n",
      "bin                    float64\n",
      "aamirkhan              float64\n",
      "aaroneckhart           float64\n",
      "aaronpaul              float64\n",
      "abbiecornish           float64\n",
      "abdellatifkechiche     float64\n",
      "abigailbreslin         float64\n",
      "action                 float64\n",
      "adambrooks             float64\n",
      "adamdriver             float64\n",
      "adamelliot             float64\n",
      "adammckay              float64\n",
      "adamsandler            float64\n",
      "                        ...   \n",
      "waynewang              float64\n",
      "wendyhiller            float64\n",
      "wernerherzog           float64\n",
      "wesanderson            float64\n",
      "wescraven              float64\n",
      "wesleysnipes           float64\n",
      "western                float64\n",
      "whitneyhouston         float64\n",
      "whoopigoldberg         float64\n",
      "willemdafoe            float64\n",
      "willferrell            float64\n",
      "williamfriedkin        float64\n",
      "williamholden          float64\n",
      "williamhurt            float64\n",
      "williampowell          float64\n",
      "williamshatner         float64\n",
      "williamwyler           float64\n",
      "willsmith              float64\n",
      "wimwenders             float64\n",
      "winonaryder            float64\n",
      "wolfgangbecker         float64\n",
      "wolfgangpetersen       float64\n",
      "woodyallen             float64\n",
      "woodyharrelson         float64\n",
      "yorgoslanthimos        float64\n",
      "yulbrynner             float64\n",
      "zacefron               float64\n",
      "zachgalifianakis       float64\n",
      "zoesaldana             float64\n",
      "zooeydeschanel         float64\n",
      "Length: 1457, dtype: object\n",
      "           budget     revenue  weekday   day  month    year  runtime  \\\n",
      "12926 20000000.00 57588485.00     5.00 13.00  12.00 2002.00   118.00   \n",
      "13054 16000000.00 43411001.00     5.00  2.00   8.00 2002.00    80.00   \n",
      "14230 14000000.00  2735731.00     4.00  5.00  12.00 1991.00    85.00   \n",
      "17361        0.00  1711475.00     1.00  1.00   8.00 2005.00    88.00   \n",
      "8369   4000000.00   572212.00     4.00  3.00   9.00 2015.00    95.00   \n",
      "\n",
      "       vote_average  vote_count  weighted_rating       ...        \\\n",
      "12926          6.30      166.00             7.00       ...         \n",
      "13054          3.70      131.00             6.98       ...         \n",
      "14230          6.90      136.00             7.00       ...         \n",
      "17361          6.00        2.00             7.00       ...         \n",
      "8369           5.00       56.00             7.00       ...         \n",
      "\n",
      "       wolfgangbecker  wolfgangpetersen  woodyallen  woodyharrelson  \\\n",
      "12926            0.00              0.00        0.00            0.00   \n",
      "13054            0.00              0.00        0.00            0.00   \n",
      "14230            0.00              0.00        0.00            0.00   \n",
      "17361            0.00              0.00        0.00            0.00   \n",
      "8369             0.00              0.00        0.00            0.00   \n",
      "\n",
      "       yorgoslanthimos  yulbrynner  zacefron  zachgalifianakis  zoesaldana  \\\n",
      "12926             0.00        0.00      0.00              0.00        0.00   \n",
      "13054             0.00        0.00      0.00              0.00        0.00   \n",
      "14230             0.00        0.00      0.00              0.00        0.00   \n",
      "17361             0.00        0.00      0.00              0.00        0.00   \n",
      "8369              0.00        0.00      0.00              0.00        0.00   \n",
      "\n",
      "       zooeydeschanel  \n",
      "12926            0.00  \n",
      "13054            0.00  \n",
      "14230            0.00  \n",
      "17361            0.00  \n",
      "8369             0.00  \n",
      "\n",
      "[5 rows x 1457 columns]\n",
      "(19560, 1457)\n"
     ]
    }
   ],
   "source": [
    "# Read from text dataframes (before PCA)\n",
    "print(dfnum.shape)\n",
    "\n",
    "dfcasts = pd.read_pickle(\"../2. Data Preprocessing/dfcasts\")\n",
    "dfcasts.columns = [x[0] for x in dfcasts.columns]\n",
    "dfcasts = dfcasts.sort_values('id')\n",
    "dfcasts.drop(['id'], 1, inplace=True)\n",
    "# print(dfcasts.sample(5))\n",
    "print(dfcasts.shape)\n",
    "\n",
    "dfdirectors = pd.read_pickle(\"../2. Data Preprocessing/dfdirectors\")\n",
    "dfdirectors.columns = [x[0] for x in dfdirectors.columns]\n",
    "dfdirectors = dfdirectors.sort_values('id')\n",
    "dfdirectors.drop(['id'], 1, inplace=True)\n",
    "# print(dfdirectors.sample(5))\n",
    "print(dfdirectors.shape)\n",
    "\n",
    "dfgenres = pd.read_pickle(\"../2. Data Preprocessing/dfgenres\")\n",
    "dfgenres.columns = [x[0] for x in dfgenres.columns]\n",
    "dfgenres = dfgenres.sort_values('id')\n",
    "dfgenres.drop(['title'], 1, inplace=True) # keep id here as genres has no missing values\n",
    "# print(dfgenres.sample(5))\n",
    "print(dfgenres.shape)\n",
    "\n",
    "# COMMENTED OUT overview & production companies because of MEMORY ERROR\n",
    "\n",
    "# dfoverview = pd.read_pickle(\"../2. Data Preprocessing/dfoverview\")\n",
    "# dfoverview.columns = [x[0] for x in dfoverview.columns]\n",
    "# dfoverview = dfoverview.sort_values('id')\n",
    "# dfoverview.drop(['id', 'title'], 1, inplace=True)\n",
    "# print(dfoverview.sample(5))\n",
    "# print(dfoverview.shape)\n",
    "\n",
    "# dfproductioncompanies = pd.read_pickle(\"../2. Data Preprocessing/dfproductioncompanies\")\n",
    "# dfproductioncompanies.columns = [x[0] for x in dfproductioncompanies.columns]\n",
    "# dfproductioncompanies = dfproductioncompanies.sort_values('id')\n",
    "# dfproductioncompanies.drop(['id', 'title'], 1, inplace=True)\n",
    "# print(dfproductioncompanies.sample(5))\n",
    "# print(dfproductioncompanies.shape)\n",
    "\n",
    "# Combine dataframes\n",
    "final_df = pd.concat([dfcasts, dfdirectors, dfgenres])\n",
    "final_df[\"id\"] = pd.to_numeric(final_df[\"id\"])\n",
    "print(final_df.shape) # 29 cols disappeared?\n",
    "# final_df = pd.concat([dfcasts, dfdirectors, dfgenres, dfoverview, dfproductioncompanies])\n",
    "final_df = pd.merge(dfnum, final_df, on='id', how='left')\n",
    "final_df = final_df.replace([np.inf, -np.inf, np.nan], 0) #removing infinite/nan values\n",
    "\n",
    "# Check the columns using dtypes\n",
    "print(final_df.dtypes)\n",
    "\n",
    "# Randomly sample 5 records with .sample(5)\n",
    "print(final_df.sample(5))\n",
    "print(final_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Find out the number of records per revenue bin. </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T13:55:55.489674Z",
     "start_time": "2018-11-02T13:55:55.227099Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Using groupby, find out the number of reviews with\n",
    "# positive and negative sentiment respectively.\n",
    "df_target = df.groupby('bin').size().reset_index(name='n')\n",
    "print(df_target)\n",
    "\n",
    "# How many patients in the dataset have been diagnosed positive and negative for diabetes?\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "df_target.plot(kind='bar', x='bin', y='n', title = \"Target class count\", ax=ax1)\n",
    "ax1.set_ylabel(\"No. of Movies\")\n",
    "plt.xticks(np.arange(0,5), [\"<35k\", \"35k to 650k\", \"650k to 800k\", \"800k to 45mil\", \">45mil\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Train-Test Split</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T13:49:14.651188Z",
     "start_time": "2018-11-02T13:49:14.642104Z"
    }
   },
   "outputs": [],
   "source": [
    "#X = df.loc[:, df.columns != 'bin']\n",
    "X = df[['budget', 'weekday', 'day', 'month', 'year', 'runtime', 'weighted_rating']]\n",
    "#X = df[['log_budget', 'weekday', 'day', 'month', 'year', 'log_runtime', 'log_weighted_rating']]\n",
    "y = df[['bin']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Baseline Classifier (Decision Tree)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'max_depth' : list(range(5, 10))\n",
    "}\n",
    "\n",
    "decisionTree = GridSearchCV(DecisionTreeClassifier(), cv=3, param_grid=parameters)\n",
    "#Fit the training feature Xs and training label Ys\n",
    "decisionTree.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "#Use the trained model to predict the test data\n",
    "y_pred = decisionTree.predict(X_test)\n",
    "\n",
    "# Find the confusion matrix, the accuracy, and F1 score of the result\n",
    "printModelAccuracy(y_test, y_pred)\n",
    "\n",
    "# Best hyperparameters to use for model\n",
    "print(\"Best Parameters:\",decisionTree.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1. k-Nearest Neighbor (KNN)</h1>\n",
    "\n",
    "Refer to the following links on for detail explanation on the implementation:\n",
    "- [kNN Classifier SKLearn Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "- [DataCamp Implementation](https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T13:59:24.193338Z",
     "start_time": "2018-11-02T13:59:24.179772Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create the kNN classifier and set the number of neighbors. Note that you can tune this number of neighbors\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "parameters = {'n_neighbors':list(range(1, 10)),\n",
    "              'leaf_size':[1,3,5],\n",
    "              'algorithm':['auto', 'kd_tree'],\n",
    "              'n_jobs':[-1]}\n",
    "\n",
    "#Fit the training feature Xs and training label Ys\n",
    "knn = GridSearchCV(\n",
    "        knn, \n",
    "        cv=3, \n",
    "        param_grid=parameters, \n",
    "        scoring='f1_macro')\n",
    "\n",
    "knn.fit(X_train,y_train.values.ravel())\n",
    "\n",
    "#Use the trained model to predict the test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Find the confusion matrix, the accuracy, and F1 score of the result\n",
    "printModelAccuracy(y_test, y_pred)\n",
    "\n",
    "# Best hyperparameters to use for model\n",
    "print(\"Best Parameters:\",knn.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. Bagging (with Decision Tree)</h1>\n",
    "\n",
    "Refer to the following links on for detail explanation on the implementation:\n",
    "- [Bagging Classifier SKLearn Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)\n",
    "\n",
    "*Note that the default AdaBoost implementation in SKLearn is Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the Bagging classifier. Default base classifiers is Decision Tree. \n",
    "# - n_estimator is the number of base classifiers (i.e. weak learners)\n",
    "parameters = {\n",
    "    'base_estimator__max_depth' : list(range(5, 10)),\n",
    "    'n_estimators' : [100, 200],\n",
    "    'max_features' : [0.5, 0.6, 0.7],\n",
    "    'max_samples' : [0.6, 0.7]\n",
    "}\n",
    "\n",
    "baggingTree = GridSearchCV(\n",
    "                BaggingClassifier(DecisionTreeClassifier()), \n",
    "                cv=3,\n",
    "                param_grid=parameters, \n",
    "                scoring='f1_macro')\n",
    "\n",
    "#Fit the training feature Xs and training label Ys\n",
    "baggingTree.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "#Use the trained model to predict the test data\n",
    "y_pred = baggingTree.predict(X_test)\n",
    "\n",
    "# Find the confusion matrix, the accuracy, and F1 score of the result\n",
    "printModelAccuracy(y_test, y_pred)\n",
    "\n",
    "# Best hyperparameters to use for model\n",
    "print(\"Best Parameters:\",baggingTree.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3. Bagging (with kNN)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the kNN base classifier\n",
    "parameters = {\n",
    "    'n_estimators' : [100, 200],\n",
    "    'max_features' : [0.5, 0.6, 0.7],\n",
    "    'max_samples' : [0.6, 0.7]\n",
    "}\n",
    "\n",
    "baggingknn = GridSearchCV(\n",
    "                BaggingClassifier(KNeighborsClassifier(algorithm='auto', leaf_size=3, n_jobs=-1, n_neighbors=9)), \n",
    "                cv=3,\n",
    "                param_grid=parameters, \n",
    "                scoring='f1_macro')\n",
    "\n",
    "#Create the Bagging classifier. Default base classifiers is Decision Tree. \n",
    "# - n_estimator is the number of base classifiers (i.e. weak learners)\n",
    "#baggingknn = BaggingClassifier(n_estimators=50, base_estimator=knn)\n",
    "\n",
    "#Fit the training feature Xs and training label Ys\n",
    "baggingknn.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "#Use the trained model to predict the test data\n",
    "y_pred = baggingknn.predict(X_test)\n",
    "\n",
    "# Find the confusion matrix, the accuracy, and F1 score of the result\n",
    "printModelAccuracy(y_test, y_pred)\n",
    "\n",
    "# Best hyperparameters to use for model\n",
    "print(\"Best Parameters:\",baggingknn.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>4. AdaBoost (with Decision Tree)</h1>\n",
    "\n",
    "Refer to the following links on for detail explanation on the implementation:\n",
    "- [AdaBoost Classifier SKLearn Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)\n",
    "- [DataCamp Implementation](https://www.datacamp.com/community/tutorials/adaboost-classifier-python)\n",
    "- [Setting Learning Rate and N Estimators](https://stats.stackexchange.com/questions/82323/shrinkage-parameter-in-adaboost)\n",
    "\n",
    "*Note that the default AdaBoost implementation in SKLearn is Decision Tree \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T13:56:38.864454Z",
     "start_time": "2018-11-02T13:56:38.853524Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create the AdaBoost classifier. Default base classifiers is Decision Tree. \n",
    "# - n_estimator is the number of base classifiers (i.e. weak learners)\n",
    "# - learning_rate controls the weight adjustments of each base classifiers. Default is 1\n",
    "# - learning_rate controls the weight adjustments of each base classifiers. Default is 1\n",
    "parameters = {\"base_estimator__max_depth\" : list(range(5, 10)),\n",
    "              \"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n",
    "              \"n_estimators\": [1, 50, 100, 200],\n",
    "              \"learning_rate\": [1, 2]\n",
    "             }\n",
    "\n",
    "adaboostTree = GridSearchCV(AdaBoostClassifier(DecisionTreeClassifier()), cv=3, param_grid=parameters)\n",
    "\n",
    "#Fit the training feature Xs and training label Ys\n",
    "adaboostTree.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "#Use the trained model to predict the test data\n",
    "y_pred = adaboostTree.predict(X_test)\n",
    "\n",
    "# Find the confusion matrix, the accuracy, and F1 score of the result\n",
    "printModelAccuracy(y_test, y_pred)\n",
    "print(\"Best Parameters:\",adaboostTree.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>5. AdaBoost (with Gaussian Navie Bayes)</h1>\n",
    "\n",
    "Refer to the following links on for detail explanation on the implementation:\n",
    "- [Gaussian Naive Bayes Classifier SKLearn Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n",
    "- [Naive Bayes Classifier video](https://www.youtube.com/watch?v=CPqOCI0ahss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naivebayes = GaussianNB()\n",
    "#Fit the training feature Xs and training label Ys\n",
    "naivebayes.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "#Use the trained model to predict the test data\n",
    "y_pred = naivebayes.predict(X_test)\n",
    "\n",
    "# Find the confusion matrix, the accuracy, and F1 score of the result\n",
    "printModelAccuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB()\n",
    "\n",
    "adaboostnaivebayes = AdaBoostClassifier(n_estimators=50,learning_rate=1, base_estimator=nb)\n",
    "#model = BaggingClassifier(n_estimators=50, base_estimator=knn)\n",
    "\n",
    "#Fit the training feature Xs and training label Ys\n",
    "adaboostnaivebayes.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "#Use the trained model to predict the test data\n",
    "y_pred = adaboostnaivebayes.predict(X_test)\n",
    "\n",
    "# Find the confusion matrix, the accuracy, and F1 score of the result\n",
    "printModelAccuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>6. Random Forest</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate model\n",
    "randomforest = RandomForestClassifier()\n",
    "\n",
    "parameters = { \n",
    "    'n_estimators': [200, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "#Fit the training feature Xs and training label Ys\n",
    "randomforest = GridSearchCV(randomforest, cv=3, param_grid=parameters, scoring='f1_macro')\n",
    "randomforest.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "#Use the trained model to predict the test data\n",
    "y_pred = randomforest.predict(X_test)\n",
    "\n",
    "# Find the confusion matrix, the accuracy, and F1 score of the result\n",
    "printModelAccuracy(y_test, y_pred)\n",
    "\n",
    "# Best hyperparameters to use for model\n",
    "print(\"Best Parameters:\", randomforest.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>7. Logistic Regression</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new logistic regression model ‘lbfgs’, ‘sag’ and ‘newton-cg’ solvers.\n",
    "log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "\n",
    "#fit the model to the training data\n",
    "log_reg.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "#Use the trained model to predict the test data\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Find the confusion matrix, the accuracy, and F1 score of the result\n",
    "printModelAccuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>8. Ensemble (Stacking with all models)</h1>\n",
    "\n",
    "As I mentioned in lecture, it is possible to ensemble different models. So how can we do that in python? Check out the following link and try it for your project!:\n",
    "https://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#knn with best parameters\n",
    "knn = KNeighborsClassifier(algorithm='auto', leaf_size=3, n_jobs=-1, n_neighbors=9)\n",
    "knn.fit(X_train,y_train.values.ravel())\n",
    "\n",
    "#baggingTree with best parameters\n",
    "baggingTree = BaggingClassifier(DecisionTreeClassifier(max_depth=8), max_features=0.7, max_samples=0.5, n_estimators=100)\n",
    "baggingTree.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "#baggingknn with best parameters\n",
    "baggingknn = BaggingClassifier(knn, max_features=0.5, max_samples=0.7, n_estimators=200)\n",
    "baggingknn.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "#Adaboost(DecisionTree) with best parameters\n",
    "adaboostTree = AdaBoostClassifier(DecisionTreeClassifier(criterion='gini', max_depth=8, splitter='best'), learning_rate=2, n_estimators=1)\n",
    "adaboostTree.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "#random forest with best parameters\n",
    "randomforest = RandomForestClassifier(criterion='gini', max_depth=6, max_features='log2', n_estimators=500)\n",
    "randomforest.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "#create a dictionary of our models\n",
    "estimators=[('knn', knn), \n",
    "            ('baggingTree', baggingTree),\n",
    "            ('baggingknn', baggingknn),\n",
    "            ('adaboostTree', adaboostTree),\n",
    "            #('naivebayes', naivebayes),\n",
    "            #('adaboostnaivebayes', adaboostnaivebayes),\n",
    "            ('randomforest', randomforest), \n",
    "            ('log_reg', log_reg)]\n",
    "\n",
    "#create our voting classifier, inputting our models, voting hard means asking classifers to make predictions by majority vote\n",
    "ensemble = VotingClassifier(estimators, voting='hard')\n",
    "\n",
    "#fit model to training data\n",
    "ensemble.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "#Use the trained model to predict the test data\n",
    "y_pred = ensemble.predict(X_test)\n",
    "\n",
    "# Find the confusion matrix, the accuracy, and F1 score of the result\n",
    "printModelAccuracy(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import all libraries and reading explored data into Dataframe</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T13:49:14.197888Z",
     "start_time": "2018-11-02T13:49:12.900261Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "\n",
    "#General libraries needed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Libraries for data pre-processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Libraries for data pre-processing (Log Loss)\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "#For Decision Tree implementation\n",
    "from scipy.stats import entropy\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "#For KNN implementation\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#For Bagging implementation\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "#For AdaBoost implementation\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#For Random Forest implementation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#For Baseline implementation\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "#For Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#For Ensemble\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#Settings\n",
    "%matplotlib inline\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "sns.set()\n",
    "\n",
    "def printModelAccuracy(y_test, y_pred):\n",
    "    # Find the confusion matrix of the result\n",
    "    cm = pd.DataFrame(confusion_matrix(y_test, y_pred, labels=[1, 2, 3, 4, 5]), \\\n",
    "        index=['true:1', 'true:2', 'true:3', 'true:4', 'true:5'], \n",
    "        columns=['pred:1', 'pred:2', 'pred:3', 'pred:4', 'pred:5'])\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Find the accuracy and F1 score of the result\n",
    "    asr = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='micro')\n",
    "    print(\"Accuracy:\", asr)\n",
    "    print(\"F1:\", f1)\n",
    "    \"\"\"\n",
    "    # Log loss\n",
    "    score = log_loss(y_test, y_pred)\n",
    "    print(\"Log Loss:\", score)\n",
    "    \"\"\"\n",
    "    \n",
    "# Read from dataframe\n",
    "dfnum = pd.read_pickle(\"../3. Exploratory Data Analysis/explored_data\")\n",
    "dfnum = dfnum.replace([np.inf, -np.inf, np.nan], 0) #removing infinite/nan values\n",
    "df = dfnum.drop(['id'], 1) # use only num data\n",
    "\n",
    "# Check the columns using dtypes\n",
    "print(df.dtypes)\n",
    "# Randomly sample 5 records with .sample(5)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Process Files for Experiment B & C</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets into one df (without PCA)\n",
    "\n",
    "# Read from text dataframes (before PCA)\n",
    "print(\"dfnum\")\n",
    "print(dfnum.shape)\n",
    "\n",
    "dfcasts = pd.read_pickle(\"../2. Data Preprocessing/dfcasts\")\n",
    "dfcasts.columns = [x[0] for x in dfcasts.columns]\n",
    "dfcasts = dfcasts.sort_values('id')\n",
    "# dfcasts.drop(['id'], 1, inplace=True)\n",
    "# print(dfcasts.sample(5))\n",
    "print(\"dfcasts\")\n",
    "print(dfcasts.shape)\n",
    "\n",
    "dfdirectors = pd.read_pickle(\"../2. Data Preprocessing/dfdirectors\")\n",
    "dfdirectors.columns = [x[0] for x in dfdirectors.columns]\n",
    "dfdirectors = dfdirectors.sort_values('id')\n",
    "# dfdirectors.drop(['id'], 1, inplace=True)\n",
    "# print(dfdirectors.sample(5))\n",
    "print(\"dfdirectors\")\n",
    "print(dfdirectors.shape)\n",
    "\n",
    "dfgenres = pd.read_pickle(\"../2. Data Preprocessing/dfgenres\")\n",
    "dfgenres.columns = [x[0] for x in dfgenres.columns]\n",
    "dfgenres = dfgenres.sort_values('id')\n",
    "dfgenres.drop(['title'], 1, inplace=True) # keep id here as genres has no missing values\n",
    "# print(dfgenres.sample(5))\n",
    "print(\"dfgenres\")\n",
    "print(dfgenres.shape)\n",
    "\n",
    "dfoverview_s = pd.read_pickle(\"../2. Data Preprocessing/dfoverviewmostcommon\")\n",
    "dfoverview_s.columns = [x[0] for x in dfoverview_s.columns]\n",
    "dfoverview_s = dfoverview_s.sort_values('id')\n",
    "dfoverview_s.drop(['title'], 1, inplace=True)\n",
    "# print(dfoverview_s.sample(5))\n",
    "print(\"dfoverview_s\")\n",
    "print(dfoverview_s.shape)\n",
    "\n",
    "dfproductioncompanies_s = pd.read_pickle(\"../2. Data Preprocessing/dfproductioncompaniesmostcommon\")\n",
    "dfproductioncompanies_s.columns = [x[0] for x in dfproductioncompanies_s.columns]\n",
    "dfproductioncompanies_s = dfproductioncompanies_s.sort_values('id')\n",
    "dfproductioncompanies_s.drop(['title'], 1, inplace=True)\n",
    "# print(dfproductioncompanies_s.sample(5))\n",
    "print(\"dfproductioncompanies_s\")\n",
    "print(dfproductioncompanies_s.shape)\n",
    "\n",
    "# COMMENTED OUT overview & production companies because of MEMORY ERROR\n",
    "\"\"\"\n",
    "dfoverview = pd.read_pickle(\"../2. Data Preprocessing/dfoverview\")\n",
    "dfoverview.columns = [x[0] for x in dfoverview.columns]\n",
    "dfoverview = dfoverview.sort_values('id')\n",
    "dfoverview.drop(['title'], 1, inplace=True)\n",
    "# print(dfoverview.sample(5))\n",
    "print(\"dfoverview\")\n",
    "print(dfoverview.shape)\n",
    "\n",
    "dfproductioncompanies = pd.read_pickle(\"../2. Data Preprocessing/dfproductioncompanies\")\n",
    "dfproductioncompanies.columns = [x[0] for x in dfproductioncompanies.columns]\n",
    "dfproductioncompanies = dfproductioncompanies.sort_values('id')\n",
    "dfproductioncompanies.drop(['title'], 1, inplace=True)\n",
    "# print(dfproductioncompanies.sample(5))\n",
    "print(\"dfproductioncompanies\")\n",
    "print(dfproductioncompanies.shape)\n",
    "\"\"\"\n",
    "\n",
    "# Combine dataframes\n",
    "casts_directors = pd.merge(dfcasts, dfdirectors, on='id', how='left')\n",
    "cd_genres = pd.merge(casts_directors, dfgenres, on='id', how='right')\n",
    "overview_productioncompanies = pd.merge(dfoverview_s, dfproductioncompanies_s, on='id', how='left')\n",
    "final_df = pd.merge(cd_genres, overview_productioncompanies, on='id', how='right')\n",
    "\n",
    "final_df[\"id\"] = pd.to_numeric(final_df[\"id\"])\n",
    "final_df = final_df.replace([np.inf, -np.inf, np.nan], 0) #removing infinite/nan values\n",
    "print(\"final_df without dfnum\")\n",
    "print(final_df.shape)\n",
    "\n",
    "final_df = pd.merge(dfnum, final_df, on='id', how='left')\n",
    "final_df = final_df.replace([np.inf, -np.inf, np.nan], 0) #removing infinite/nan values\n",
    "print(\"final_df with dfnum\")\n",
    "print(final_df.shape)\n",
    "\n",
    "final_df.to_pickle(\"withoutpca_data\")\n",
    "\n",
    "final_df = final_df.drop(['budget_x', 'vote_count'], 1)\n",
    "final_df.to_pickle(\"withoutpca_textdata\")\n",
    "\n",
    "# Check the columns using dtypes\n",
    "print(final_df.dtypes)\n",
    "\n",
    "# Randomly sample 5 records with .sample(5)\n",
    "print(final_df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Read Experiment A</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use categorical & numerical attributes only\n",
    "df = pd.read_pickle(\"../3. Exploratory Data Analysis/explored_data\")\n",
    "df = df.drop(['id'], 1)\n",
    "\n",
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Read Experiment B</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use textual attributes only\n",
    "df = pd.read_pickle(\"withoutpca_textdata\")\n",
    "df = df.drop(['id'], 1)\n",
    "\n",
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Read Experiment C</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use concatenation of (1) categorical & numerical attributes, and (2) TF-IDF vectors\n",
    "df = pd.read_pickle(\"withoutpca_data\")\n",
    "df = df.drop(['id'], 1)\n",
    "\n",
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Read Experiment D</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use concatenation of (1) categorical & numerical attributes, \n",
    "# and (2) lower dimension vector after performing dimension reduction on original TF-IDF vector\n",
    "df = pd.read_pickle(\"../3. Exploratory Data Analysis/pca_data\")\n",
    "df = df.drop(['id'], 1)\n",
    "\n",
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Find out the number of records per revenue bin. </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T13:55:55.489674Z",
     "start_time": "2018-11-02T13:55:55.227099Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Using groupby, find out the number of reviews with\n",
    "# positive and negative sentiment respectively.\n",
    "#df_target = df.groupby('bin').size().reset_index(name='n')\n",
    "#print(df_target)\n",
    "\n",
    "# How many patients in the dataset have been diagnosed positive and negative for diabetes?\n",
    "#fig = plt.figure(figsize=(6, 6))\n",
    "#ax1 = fig.add_subplot(111)\n",
    "#df_target.plot(kind='bar', x='bin', y='n', title = \"Target class count\", ax=ax1)\n",
    "#ax1.set_ylabel(\"No. of Movies\")\n",
    "#plt.xticks(np.arange(0,5), [\"<35k\", \"35k to 650k\", \"650k to 800k\", \"800k to 45mil\", \">45mil\"])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Train-Test Split</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T13:49:14.651188Z",
     "start_time": "2018-11-02T13:49:14.642104Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df.loc[:, df.columns != 'bin']\n",
    "y = df[['bin']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_pickle(\"X_train\")\n",
    "X_test.to_pickle(\"X_test\")\n",
    "y_train.to_pickle(\"y_train\")\n",
    "y_test.to_pickle(\"y_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from dataframe    \n",
    "X_test = pd.read_pickle(\"X_test\")\n",
    "X_train = pd.read_pickle(\"X_train\")\n",
    "y_test = pd.read_pickle(\"y_test\")\n",
    "y_train = pd.read_pickle(\"y_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Baseline Classifier (Decision Tree)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'criterion': ['gini', 'entropy'] #entropy better than gini\n",
    "}\n",
    "\n",
    "decisionTree = GridSearchCV(DecisionTreeClassifier(), cv=5, param_grid=parameters)\n",
    "#Fit the training feature Xs and training label Ys\n",
    "decisionTree.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "#Use the trained model to predict the test data\n",
    "y_pred = decisionTree.predict(X_test)\n",
    "\n",
    "# Find the confusion matrix, the accuracy, and F1 score of the result\n",
    "printModelAccuracy(y_test, y_pred)\n",
    "\n",
    "# Best hyperparameters to use for model\n",
    "print(\"Best Parameters:\",decisionTree.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import all libraries and reading explored data into Dataframe</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T13:49:14.197888Z",
     "start_time": "2018-11-02T13:49:12.900261Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['id'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a875f2792995>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[0mdfnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../3. Exploratory Data Analysis/explored_data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[0mdfnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdfnum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#removing infinite/nan values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdfnum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;31m# Check the columns using dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3695\u001b[0m                                            \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3696\u001b[0m                                            \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3697\u001b[1;33m                                            errors=errors)\n\u001b[0m\u001b[0;32m   3698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3699\u001b[0m     @rewrite_axis_style_signature('mapper', [('copy', True),\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3109\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3110\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3111\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3141\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3142\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3143\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3144\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   4402\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'ignore'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4403\u001b[0m                 raise KeyError(\n\u001b[1;32m-> 4404\u001b[1;33m                     '{} not found in axis'.format(labels[mask]))\n\u001b[0m\u001b[0;32m   4405\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4406\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['id'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import re\n",
    "import io\n",
    "\n",
    "#General libraries needed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Libraries for data pre-processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Libraries for data pre-processing (Log Loss)\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "#For Decision Tree implementation\n",
    "from scipy.stats import entropy\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "#For KNN implementation\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#For Bagging implementation\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "#For AdaBoost implementation\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#For Random Forest implementation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#For Baseline implementation\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "#For Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#For Ensemble\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#Settings\n",
    "%matplotlib inline\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "sns.set()\n",
    "\n",
    "def printModelAccuracy(y_test, y_pred):\n",
    "    # Find the confusion matrix of the result\n",
    "    cm = pd.DataFrame(confusion_matrix(y_test, y_pred, labels=[1, 2, 3, 4, 5]), \\\n",
    "        index=['true:1', 'true:2', 'true:3', 'true:4', 'true:5'], \n",
    "        columns=['pred:1', 'pred:2', 'pred:3', 'pred:4', 'pred:5'])\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Find the accuracy and F1 score of the result\n",
    "    asr = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='micro')\n",
    "    print(\"Accuracy:\", asr)\n",
    "    print(\"F1:\", f1)\n",
    "    \"\"\"\n",
    "    # Log loss\n",
    "    score = log_loss(y_test, y_pred)\n",
    "    print(\"Log Loss:\", score)\n",
    "    \"\"\"\n",
    "    \n",
    "# Read from dataframe\n",
    "dfnum = pd.read_pickle(\"../3. Exploratory Data Analysis/explored_data\")\n",
    "dfnum = dfnum.replace([np.inf, -np.inf, np.nan], 0) #removing infinite/nan values\n",
    "df = dfnum.drop(['id'], 1)\n",
    "\n",
    "# Check the columns using dtypes\n",
    "print(df.dtypes)\n",
    "# Randomly sample 5 records with .sample(5)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree in SKLearn don't take in string well. So we use a label encoder to change that string to a numeric value\n",
    "\"\"\"\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == type(object):\n",
    "        #Create the label encoder\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        #Convert the non numeric data to numeric\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets into one df (without PCA)\n",
    "\n",
    "# Read from text dataframes (before PCA)\n",
    "print(dfnum.shape)\n",
    "\n",
    "dfcasts = pd.read_pickle(\"../2. Data Preprocessing/dfcasts\")\n",
    "dfcasts.columns = [x[0] for x in dfcasts.columns]\n",
    "dfcasts = dfcasts.sort_values('id')\n",
    "dfcasts.drop(['id'], 1, inplace=True)\n",
    "# print(dfcasts.sample(5))\n",
    "print(dfcasts.shape)\n",
    "\n",
    "dfdirectors = pd.read_pickle(\"../2. Data Preprocessing/dfdirectors\")\n",
    "dfdirectors.columns = [x[0] for x in dfdirectors.columns]\n",
    "dfdirectors = dfdirectors.sort_values('id')\n",
    "dfdirectors.drop(['id'], 1, inplace=True)\n",
    "# print(dfdirectors.sample(5))\n",
    "print(dfdirectors.shape)\n",
    "\n",
    "dfgenres = pd.read_pickle(\"../2. Data Preprocessing/dfgenres\")\n",
    "dfgenres.columns = [x[0] for x in dfgenres.columns]\n",
    "dfgenres = dfgenres.sort_values('id')\n",
    "dfgenres.drop(['title'], 1, inplace=True) # keep id here as genres has no missing values\n",
    "# print(dfgenres.sample(5))\n",
    "print(dfgenres.shape)\n",
    "\n",
    "dfoverview_s = pd.read_pickle(\"../2. Data Preprocessing/dfoverviewmostcommon\")\n",
    "dfoverview_s.columns = [x[0] for x in dfoverview_s.columns]\n",
    "dfoverview_s = dfoverview_s.sort_values('id')\n",
    "dfoverview_s.drop(['id', 'title'], 1, inplace=True)\n",
    "# print(dfoverview_s.sample(5))\n",
    "print(dfoverview_s.shape)\n",
    "\n",
    "dfproductioncompanies_s = pd.read_pickle(\"../2. Data Preprocessing/dfproductioncompaniesmostcommon\")\n",
    "dfproductioncompanies_s.columns = [x[0] for x in dfproductioncompanies_s.columns]\n",
    "dfproductioncompanies_s = dfproductioncompanies_s.sort_values('id')\n",
    "dfproductioncompanies_s.drop(['id', 'title'], 1, inplace=True)\n",
    "# print(dfproductioncompanies_s.sample(5))\n",
    "print(dfproductioncompanies_s.shape)\n",
    "\n",
    "# COMMENTED OUT overview & production companies because of MEMORY ERROR\n",
    "\n",
    "# dfoverview = pd.read_pickle(\"../2. Data Preprocessing/dfoverview\")\n",
    "# dfoverview.columns = [x[0] for x in dfoverview.columns]\n",
    "# dfoverview = dfoverview.sort_values('id')\n",
    "# dfoverview.drop(['id', 'title'], 1, inplace=True)\n",
    "# # print(dfoverview.sample(5))\n",
    "# print(dfoverview.shape)\n",
    "\n",
    "# dfproductioncompanies = pd.read_pickle(\"../2. Data Preprocessing/dfproductioncompanies\")\n",
    "# dfproductioncompanies.columns = [x[0] for x in dfproductioncompanies.columns]\n",
    "# dfproductioncompanies = dfproductioncompanies.sort_values('id')\n",
    "# dfproductioncompanies.drop(['id', 'title'], 1, inplace=True)\n",
    "# # print(dfproductioncompanies.sample(5))\n",
    "# print(dfproductioncompanies.shape)\n",
    "\n",
    "# Combine dataframes - MEMORY ERROR\n",
    "#final_df = pd.concat([dfcasts, dfdirectors, dfgenres])\n",
    "final_df = pd.concat([dfcasts, dfdirectors, dfgenres, dfoverview_s, dfproductioncompanies_s])\n",
    "final_df[\"id\"] = pd.to_numeric(final_df[\"id\"])\n",
    "print(final_df.shape) # 29 cols disappeared?\n",
    "final_df.to_pickle(\"withoutpca_textdata\")\n",
    "# final_df = pd.concat([dfcasts, dfdirectors, dfgenres, dfoverview_s, dfproductioncompanies])\n",
    "final_df = pd.merge(dfnum, final_df, on='id', how='left')\n",
    "final_df = final_df.replace([np.inf, -np.inf, np.nan], 0) #removing infinite/nan values\n",
    "\n",
    "# Check the columns using dtypes\n",
    "print(final_df.dtypes)\n",
    "\n",
    "# Randomly sample 5 records with .sample(5)\n",
    "print(final_df.sample(5))\n",
    "print(final_df.shape)\n",
    "\n",
    "final_df.to_pickle(\"withoutpca_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA data\n",
    "dfpca = pd.read_pickle(\"../3. Exploratory Data Analysis/pca_data\")\n",
    "# dfpca = dfpca.replace([np.inf, -np.inf, np.nan], 0) #removing infinite/nan values\n",
    "dfpca = dfpca.drop(['id'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpca.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Find out the number of records per revenue bin. </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T13:55:55.489674Z",
     "start_time": "2018-11-02T13:55:55.227099Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Using groupby, find out the number of reviews with\n",
    "# positive and negative sentiment respectively.\n",
    "df_target = df.groupby('bin').size().reset_index(name='n')\n",
    "print(df_target)\n",
    "\n",
    "# How many patients in the dataset have been diagnosed positive and negative for diabetes?\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "df_target.plot(kind='bar', x='bin', y='n', title = \"Target class count\", ax=ax1)\n",
    "ax1.set_ylabel(\"No. of Movies\")\n",
    "plt.xticks(np.arange(0,5), [\"<35k\", \"35k to 650k\", \"650k to 800k\", \"800k to 45mil\", \">45mil\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Train-Test Split</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T13:49:14.651188Z",
     "start_time": "2018-11-02T13:49:14.642104Z"
    }
   },
   "outputs": [],
   "source": [
    "dfpca2 = dfpca.drop(['log_revenue', 'log_budget', 'log_runtime', 'log_vote_average', 'log_vote_count', 'log_weighted_rating', 'revenue', 'vote_average', 'vote_count'], 1)\n",
    "\n",
    "X = dfpca2.loc[:, dfpca2.columns != 'bin']\n",
    "#X = final_df[['budget', 'weekday', 'day', 'month', 'year', 'runtime', 'weighted_rating']]\n",
    "#X = final_df[['log_budget', 'weekday', 'day', 'month', 'year', 'log_runtime', 'log_weighted_rating']]\n",
    "y = dfpca2[['bin']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_pickle(\"X_train\")\n",
    "X_test.to_pickle(\"X_test\")\n",
    "y_train.to_pickle(\"y_train\")\n",
    "y_test.to_pickle(\"y_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Baseline Classifier (Decision Tree)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'max_depth' : list(range(5, 10))\n",
    "}\n",
    "\n",
    "decisionTree = GridSearchCV(DecisionTreeClassifier(), cv=3, param_grid=parameters)\n",
    "#Fit the training feature Xs and training label Ys\n",
    "decisionTree.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "#Use the trained model to predict the test data\n",
    "y_pred = decisionTree.predict(X_test)\n",
    "\n",
    "# Find the confusion matrix, the accuracy, and F1 score of the result\n",
    "printModelAccuracy(y_test, y_pred)\n",
    "\n",
    "# Best hyperparameters to use for model\n",
    "print(\"Best Parameters:\",decisionTree.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
